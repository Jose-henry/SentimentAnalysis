# -*- coding: utf-8 -*-
"""text-classificationBasedOnSentiments-Hugging face-xlm-roberta-base.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MYgwFRLHu1mG_ZSijF2ikZhnyatQuZL3

**GETTING AND UNDERSTANDING YOUR DATASETS**
"""

## pip install -q datasets # installing huggings face dataset

from datasets import load_dataset # to read and load cdatasets
emotions = load_dataset("cardiffnlp/tweet_sentiment_multilingual", 'all') ## assigning emotion dataset to emotions variable

emotions

train_set = emotions['train'] ## setting the train dataset in the dict to train_set
train_set

train_set[3]

"""REMOVING UNWANTED STRINGS FROM OUR DATA"""

import re

# Define a function to clean the text
def clean_text(text):
    text = re.sub('RT', '', text)  # Remove 'RT'
    text = re.sub('@user', '', text)  # Remove '@user'
    text = re.sub(':', '', text)  # Remove ':'
    text = re.sub('http', '', text)  # Remove 'http'
    text = text.strip()  # Remove leading and trailing white spaces
    return text

# Apply the function to the 'text' column of each dataset
emotions['train'] = emotions['train'].map(lambda examples: {'text': clean_text(examples['text'])})
emotions['test'] = emotions['test'].map(lambda examples: {'text': clean_text(examples['text'])})
emotions['validation'] = emotions['validation'].map(lambda examples: {'text': clean_text(examples['text'])})

train_set = emotions['train'] ## setting the train dataset in the dict to train_set
train_set

train_set[0]

"""text: a string feature containing the tweet.

label: an int classification label with the following mapping:

0: negative

1: neutral

2: positive
"""

##converting dataset to dataframe
emotions.set_format(type="pandas")
df = emotions["train"][:]
df.head()

##getting the label of classification
def label_converter(row):
  return emotions['train'].features['label'].int2str(row)
df['label_name'] = df['label'].apply(label_converter)
df.head()

import matplotlib.pyplot as plt
import seaborn as sns
sns.set_theme()

df["label_name"].value_counts(ascending=True).plot.barh()
plt.title('Frequency of classes')
plt.show()

emotions.reset_format() ## to balance datasets

"""**DATA PREPROCESSING,(TOKENIZATION)**"""

#pip install -q transformers -U ## -q means to suppress non-error installation progress. -U means install and upgrade to latest

## Using xlm-roberta-base model for tokenization
from transformers import AutoTokenizer

checkpoint = "xlm-roberta-base"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)



def tokenize_function(batch):  # applying tokenizer to a batch of data
  return tokenizer(batch["text"], padding =True, truncation =True) #padding will pad the data with zeros to the size of the longest one in a batch
  # truncation will truncate the examples to the models max content size

print(tokenize_function(emotions['train'][:2]))

tokenized_datasets = emotions.map(tokenize_function,
                                  batched = True,
                                  batch_size = None)

print(tokenized_datasets['train'].column_names) ##input_ids represent numeric representation of tokens,
#attentionmask used in model to ignore padded areas of dataset

"""**MODELLING(DistillBert)**"""

## Using a pretrained model, xlm-roberta-base

from transformers import AutoModelForSequenceClassification
import torch


checkpoint = "xlm-roberta-base"
device = torch.device('cuda' if torch.cuda.is_available() else "cpu")

model = (AutoModelForSequenceClassification.from_pretrained(checkpoint,
                                                            num_labels = 6).to(device))

"""1. Performance Metrics(Loss Function):Creating the compute_metrics object"""

from sklearn.metrics import accuracy_score, f1_score

def compute_metrics(pred):
  labels = pred.label_ids
  preds = pred.predictions.argmax(-1)
  f1 = f1_score(labels, preds, average="weighted")
  acc = accuracy_score(labels, preds)
  return {"accuracy": acc, "f1": f1}

## logging in the huggings face hub to get API key
from huggingface_hub import notebook_login
notebook_login()

#pip install -U accelerate

#pip install -U transformers

import accelerate
import transformers

transformers.__version__, accelerate.__version__

### TO RESTART KERNEL
#import os
#os._exit(00)

"""2. SPECIFYING PARAMETERS FOR TRAINING THE MODEL: Creating Trainer Arguments/Parameters Objects"""

## specifying parameters for training model
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir='Finetuning-SENTIMENT-Analysis-Model',
    num_train_epochs=2,
    per_device_train_batch_size=64,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    push_to_hub=True,
    report_to = "none",
    evaluation_strategy="epoch",
    hub_model_id="sirjosephenry/Finetuning-SENTIMENT-Analysis-Model",  # Corrected model ID
    hub_token="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",  # Your Hugging Face token
)

"""3. CREATING THE TRAINER OBJECT FOR TRAINING"""

from transformers import Trainer


trainer = Trainer(
    model = model,
    args = training_args,
    compute_metrics = compute_metrics,
    train_dataset = tokenized_datasets['train'],
    eval_dataset = tokenized_datasets['validation'],
    tokenizer = tokenizer
)


"""4. TRAINING OR FINETUNING AN ALREADY PRETRAINED MODEL FOR SENTIMENT ANALYSIS"""

trainer.train()

## saving model to hugging face hub
trainer.push_to_hub(commit_message = "Training completed")

"""**PUSHING MODEL TO PIPELINE(API) FOR CLASSIFICATION**"""

from transformers import pipeline

classifier = pipeline("text-classification",
                      model= "sirjosephenry/Finetuning-SENTIMENT-Model"
                      )

"""**CLASSIFYING TEXT BASED ON SENTIMENTS**"""

pred = classifier(["i love joseph"],
                  return_all_scores = True)

"""**PLOT PROBABILITY OF EACH CLASS ON A BAR PLOT**

> Indented block


"""

import pandas as pd

labels = emotions["train"].features["label"].names
df = pd.DataFrame(pred[0])
plt.bar(labels, 100*df["score"])
plt.show()

"""**ALREADY FINETUNED MODEL BY CARDIFF**"""

##pip install tweetnlp

# import tweetnlp
# model = tweetnlp.Classifier("cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual", max_length=128)
# model.predict('Get the all-analog Classic Vinyl Edition of "Takin Off" Album from {@herbiehancock@} via {@bluenoterecords@} link below {{URL}}')